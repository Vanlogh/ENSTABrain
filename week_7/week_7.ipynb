{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Week 7: So toxic!</font>\n",
    "\n",
    "## <font color='blue'>Provided by:</font>\n",
    "\n",
    "![enstabrain](images/ENSTABrain_gd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda:\n",
    "\n",
    "- What are NLP, sentiment analysis and text classification?\n",
    "- How can Deep Learning help in such tasks?\n",
    "- Motivation and Goal Architecture\n",
    "- Convolutional neural networks example\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP (Natural Language Processing):\n",
    "\n",
    "Natural Language Processing, or NLP for short, is broadly defined as the automatic manipulation of natural language, like speech and text, by software.\n",
    "\n",
    "The study of natural language processing has been around for more than 50 years and grew out of the field of linguistics with the rise of computers.\n",
    "\n",
    "## Natural Language\n",
    "\n",
    "Natural language refers to the way we, humans, communicate with each other.\n",
    "\n",
    "Namely, speech and text.\n",
    "\n",
    "We are surrounded by text.\n",
    "\n",
    "Think about how much text you see each day:\n",
    "\n",
    "    Signs\n",
    "    Menus\n",
    "    Email\n",
    "    SMS\n",
    "    Web Pages\n",
    "    and so much more…\n",
    "\n",
    "The list is endless.\n",
    "\n",
    "Now think about speech.\n",
    "\n",
    "We may speak to each other, as a species, more than we write. It may even be easier to learn to speak than to write.\n",
    "\n",
    "Voice and text are how we communicate with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language is primarily hard because it is messy. There are few rules.\n",
    "\n",
    "And yet we can easily understand each other most of the time.\n",
    "\n",
    "Human language is highly ambiguous … It is also ever changing and evolving. People are great at producing language and understanding language, and are capable of expressing, perceiving, and interpreting very elaborate and nuanced meanings. At the same time, while we humans are great users of language, we are also very poor at formally understanding and describing the rules that govern language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLP](images/NLP.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role of Deep Learning\n",
    "\n",
    "As seen previously in our neural networks architecture, neural networks, due to their architecture are able to ignore some features that it finds useless (especially with the help of l1 regularization).\n",
    "\n",
    "ANNs are known for their independency as they don't need much preprocessing or data preparation in order to give acceptable results.\n",
    "\n",
    "**Quick Reminder:** [here](http://playground.tensorflow.org/#activation=relu&regularization=L1&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.001&regularizationRate=0.003&noise=0&networkShape=6,5&seed=0.87378&showTestData=false&discretize=false&percTrainData=70&x=true&y=true&xTimesY=true&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "![ANN](images/ANN.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation of this series of sessions to come:\n",
    "\n",
    "While working on the AI project held at ENSTAB, we came across the [Yelp Dataset Challenge](https://www.yelp.com/dataset/challenge) where we tried implementing our own neural network and compete with existing papers on the same dataset.\n",
    "\n",
    "The dataset is about classifying (rating) reviews written by humans, mostly in english language.\n",
    "\n",
    "After running some tests, we were surprised by the fact that we achieved great results that surpassed the existing papers we found.\n",
    "\n",
    "![acc](images/acc.png)\n",
    "\n",
    "![pred1](images/pred1.png)\n",
    "\n",
    "![pred2](images/pred2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model architecture looked something like this:\n",
    "\n",
    "![Architecture](images.architecture.png)\n",
    "\n",
    "As stated in the model architecture, we need to discover many concepts in order to recreate the model:\n",
    "\n",
    "- Word/Character embeddings\n",
    "- CNNs (Convolutional neural networks) (this session)\n",
    "- RNNs (Recurrent neural networks)\n",
    "- Seq2Seq (sequence to sequence (shortly))\n",
    "- DNNs (Deep neural networks)\n",
    "\n",
    "These concepts will be held during these future sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs (Convolutional neural networks) on MNIST example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to CNNs\n",
    "\n",
    "Convolutional neural networks (CNNs) are the current state-of-the-art model architecture for image classification tasks. CNNs apply a series of filters to the raw pixel data of an image to extract and learn higher-level features, which the model can then use for classification. CNNs contains three components:\n",
    "\n",
    "- Convolutional layers, which apply a specified number of convolution filters to the image. For each subregion, the layer performs a set of mathematical operations to produce a single value in the output feature map. Convolutional layers then typically apply a ReLU activation function to the output to introduce nonlinearities into the model.\n",
    "\n",
    "- Pooling layers, which downsample the image data extracted by the convolutional layers to reduce the dimensionality of the feature map in order to decrease processing time. A commonly used pooling algorithm is max pooling, which extracts subregions of the feature map (e.g., 2x2-pixel tiles), keeps their maximum value, and discards all other values.\n",
    "\n",
    "- Dense (fully connected) layers, which perform classification on the features extracted by the convolutional layers and downsampled by the pooling layers. In a dense layer, every node in the layer is connected to every node in the preceding layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, let's talk a little bit about the intention here.**\n",
    "\n",
    "The original neural network architecture is based on feed forward fully connected layers (eg: Perceptron). However, for tasks like image processing where the inputs are raw pixels, neural networks showed to have difficulty learning directly from these raw pixels values.\n",
    "\n",
    "Thus the creation of CNNs that will create higher-level features, starting from raw pixels. These high level features will then be consumed by a FC neural network to predict a specific class.\n",
    "\n",
    "![conv](images/conv.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Let's start with the simplest\n",
    "\n",
    "## Dense layers\n",
    "\n",
    "Simple fully connected layers (revisit them in previous deep learning session)\n",
    "\n",
    "![Dense](images/Dense.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**then comes the CNNs**\n",
    "\n",
    "hard words:\n",
    "\n",
    "\n",
    "**Overview and intuition without brain stuff:** \n",
    "\n",
    "Lets first discuss what the CONV layer computes without brain/neuron analogies. The CONV layer’s parameters consist of a set of learnable filters. Every filter is small spatially (along width and height), but extends through the full depth of the input volume. For example, a typical filter on a first layer of a ConvNet might have size 5x5x3 (i.e. 5 pixels width and height, and 3 because images have depth 3, the color channels). During the forward pass, we slide (more precisely, convolve) each filter across the width and height of the input volume and compute dot products between the entries of the filter and the input at any position. As we slide the filter over the width and height of the input volume we will produce a 2-dimensional activation map that gives the responses of that filter at every spatial position. Intuitively, the network will learn filters that activate when they see some type of visual feature such as an edge of some orientation or a blotch of some color on the first layer, or eventually entire honeycomb or wheel-like patterns on higher layers of the network. Now, we will have an entire set of filters in each CONV layer (e.g. 12 filters), and each of them will produce a separate 2-dimensional activation map. We will stack these activation maps along the depth dimension and produce the output volume.\n",
    "\n",
    "**The brain view:**\n",
    "\n",
    "If you’re a fan of the brain/neuron analogies, every entry in the 3D output volume can also be interpreted as an output of a neuron that looks at only a small region in the input and shares parameters with all neurons to the left and right spatially (since these numbers all result from applying the same filter). We now discuss the details of the neuron connectivities, their arrangement in space, and their parameter sharing scheme.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Easy words (listen to the teacher)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![depthcol](images/depthcol.jpeg) ![neuron_model](images/neuron_model.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Visual Simulation ** [here](http://cs231n.github.io/convolutional-networks/#conv)\n",
    "\n",
    "**example**\n",
    "\n",
    "![conv_out](images/conv_out.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Finally the Pooling layer **\n",
    "\n",
    "** hard words: **\n",
    "\n",
    "It is common to periodically insert a Pooling layer in-between successive Conv layers in a ConvNet architecture. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting. The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation. The most common form is a pooling layer with filters of size 2x2 applied with a stride of 2 downsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations. Every MAX operation would in this case be taking a max over 4 numbers (little 2x2 region in some depth slice)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Easy words: (listen to me $@!#?)**</font>\n",
    "\n",
    "![maxpool](images/maxpool.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's code this up real quick**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a model to classify the images in the MNIST dataset using the following CNN architecture:\n",
    "- **Convolutional Layer #1**: Applies 32 5x5 filters (extracting 5x5-pixel subregions), with ReLU activation function\n",
    "- **Pooling Layer #1**: Performs max pooling with a 2x2 filter and stride of 2 (which specifies that pooled regions do not overlap)\n",
    "- **Convolutional Layer #2**: Applies 64 5x5 filters, with ReLU activation function\n",
    "- **Pooling Layer #2**: Again, performs max pooling with a 2x2 filter and stride of 2\n",
    "- **Dense Layer #1**: 1,024 neurons, with dropout regularization rate of 0.4 (probability of 0.4 that any given element will be dropped during training)\n",
    "- **Dense Layer #2 (Logits Layer)**: 10 neurons, one for each digit target class (0–9).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    \n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=input_layer,\n",
    "        filters=32,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional Layer #2 and Pooling Layer #2\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=pool1,\n",
    "        filters=64,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Dense Layer\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(\n",
    "       inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "    predictions = {\n",
    "        # Generate predictions (for PREDICT and EVAL mode)\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "        # `logging_hook`.\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(unused_argv):\n",
    "    # Load training and eval data\n",
    "    mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "    train_data = mnist.train.images # Returns np.array\n",
    "    train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "    eval_data = mnist.test.images # Returns np.array\n",
    "    eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "    # Create the Estimator\n",
    "    mnist_classifier = tf.estimator.Estimator(\n",
    "        model_fn=cnn_model_fn, model_dir=\"mnist_convnet_model\")\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": train_data},\n",
    "        y=train_labels,\n",
    "        batch_size=100,\n",
    "        num_epochs=None,\n",
    "        shuffle=True)\n",
    "    mnist_classifier.train(\n",
    "        input_fn=train_input_fn,\n",
    "        steps=20000)\n",
    "\n",
    "    # Evaluate the model and print results\n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": eval_data},\n",
    "        y=eval_labels,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "    eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "    print(eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After **10 mins** of training **(GPU)** or a couple of **hours** **(CPU)**, the model should have around 97% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "- [tensorflow tutorial](https://www.tensorflow.org/tutorials/deep_cnn)\n",
    "- [Convolutional neural network explained](http://cs231n.github.io/convolutional-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks for your attention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
